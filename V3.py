"""
V3 is leaps and bounds better than my first two versions. The first big change is using numpy arrays, which makes the code run much faster, 
so it is a lot easier for me to test it on data sets with a known optimum. I also made sure that there was no one swap per iteration, which 
made a big difference in my local search procedure. It is much easier to see the algorithm traversing through peaks and valleys. Furthermore, 
I made the output much sleeker and easier to read, and added a graph at the end to visualize the progress the algorithm made throughout each
iteration and each run. Additionally, I added in a penalty matrix, which has a similar function to the probability matrix from the first two 
versions, which is to diversify the search in terms of generating a starting solution only (the original distance matrix is used in the local search 
procedure).

"""

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time

def solve_pMedian(filename, nLOC, maxIT, tenure, runs, no_improvement_limit, seed, penalty_fraction, known_optimal=None):
    """
    Main function to solve the p-Median problem using Tabu Search.
    
    Parameters:
    filename (str): Path to the file containing the cost matrix.
    nLOC (int): Number of locations to select.
    maxIT (int): Maximum number of iterations per run.
    tenure (int): Tabu tenure (number of iterations a move remains tabu).
    runs (int): Number of runs to perform.
    no_improvement_limit (int): Maximum number of iterations without improvement before termination.
    seed (int): Random seed for reproducibility.
    penalty_fraction (float): Fraction of the maximum distance used to calculate penalties.
    known_optimal (float): Known optimal solution value, if available.
    """
    start_time = time.time()  # Record the start time to calculate the total runtime later

    def read_costMatrix():
        """Reads the cost matrix from the given filename."""
        return np.loadtxt(filename)

    def compute_objective(solution, distances):
        """
        Computes the objective value of a given solution.
        
        Parameters:
        solution (list or set): List or set of selected locations.
        distances (ndarray): Cost matrix.
        
        Returns:
        float: Objective value of the solution.
        """
        solution_list = list(solution) if isinstance(solution, set) else solution
        # Calculate the objective value as the sum of the minimum distances from each demand point to the nearest facility in the solution
        return np.min(distances[solution_list], axis=0).sum()

    def greedy_add(distances, nLOC, penalty_matrix):
        """
        Constructs a starting solution using the greedy-add algorithm.
        
        Parameters:
        distances (ndarray): Cost matrix.
        nLOC (int): Number of locations to select.
        penalty_matrix (ndarray): Penalty matrix to diversify the search.
        
        Returns:
        list: Greedy-add solution.
        """
        penalized_distances = distances + penalty_matrix  # Add penalties to the distances to diversify the search
        selected_locations = set()
        nRow = distances.shape[0]  # Number of rows (locations)

        # Iteratively add the best location that minimizes the objective function
        while len(selected_locations) < nLOC:
            remaining_locations = set(range(nRow)) - selected_locations
            best_addition = min(remaining_locations, key=lambda loc: compute_objective(selected_locations.union({loc}), penalized_distances))
            selected_locations.add(best_addition)

        selected_locations = list(selected_locations)
        print(f'The starting solution is: {selected_locations} and its objective value is {compute_objective(selected_locations, distances):.2f}')
        return selected_locations

    def construct_starting_solution(distances, nLOC, run, penalty_matrix):
        """
        Constructs a starting solution, either using the greedy-add algorithm or a random selection.
        
        Parameters:
        distances (ndarray): Cost matrix.
        nLOC (int): Number of locations to select.
        run (int): Current run number.
        penalty_matrix (ndarray): Penalty matrix to diversify the search.
        
        Returns:
        tuple: Starting solution and a boolean indicating if it was generated by the greedy-add algorithm.
        """
        nRow = distances.shape[0]  # Number of rows (locations)
        if run == 0 or random.random() >= 0.2:
            # Use greedy-add solution for the first run or with 80% probability for subsequent runs
            print('Greedy add solution')
            return greedy_add(distances, nLOC, penalty_matrix), True
        else:
            # Use a random solution with 20% probability for subsequent runs
            print('Random solution')
            randSolution = random.sample(range(nRow), nLOC)
            print(f'The starting solution is: {randSolution} and its objective value is {compute_objective(randSolution, distances):.2f}')
            return randSolution, False

    def make_tabu(stmMatrix, swap, iteration):
        """
        Marks a swap as tabu in the short-term memory (STM) matrix.
        
        Parameters:
        stmMatrix (ndarray): Short-term memory (STM) matrix.
        swap (tuple): Tuple representing the swap (out, in).
        iteration (int): Current iteration number.
        """
        i, j = swap
        stmMatrix[i, j] = stmMatrix[j, i] = iteration + tenure  # Set the tabu tenure for the swap

    def is_tabu(stmMatrix, swap, iteration):
        """
        Checks if a swap is tabu.
        
        Parameters:
        stmMatrix (ndarray): Short-term memory (STM) matrix.
        swap (tuple): Tuple representing the swap (out, in).
        iteration (int): Current iteration number.
        
        Returns:
        bool: True if the swap is tabu, False otherwise.
        """
        i, j = swap
        return stmMatrix[i, j] > iteration or stmMatrix[j, i] > iteration

    def tabu_search(distances, run, best_evaluation_location, best_evaluation, best_solution, stmMatrix, penalty_matrix):
        """
        Performs the tabu search algorithm to find the best solution.
        
        Parameters:
        distances (ndarray): Cost matrix.
        run (int): Current run number.
        best_evaluation_location (tuple): Location of the best evaluation (iteration, run).
        best_evaluation (float): Best evaluation found so far.
        best_solution (list): Best solution found so far.
        stmMatrix (ndarray): Short-term memory (STM) matrix.
        penalty_matrix (ndarray): Penalty matrix to diversify the search.
        
        Returns:
        dict: Dictionary containing details of the best solution found in the current run.
        """
        nRow = distances.shape[0]  # Number of rows (locations)
        solution, is_greedy = construct_starting_solution(distances, nLOC, run, penalty_matrix)
        inList = set(solution)
        outList = set(range(nRow)) - inList
        best_local_solution = inList.copy()
        best_local_obj = compute_objective(best_local_solution, distances)
        aspiration_count = 0
        no_improvement_iterations = 0
        last_improvement_obj = best_local_obj

        run_data = []

        # Print headers for the iteration log
        print(f"\n{'Iteration':<10}{'Swap (Out -> In)':<20}{'Iteration Obj':<20}{'Best Local Obj':<20}{'Best Global Obj':<20}{'Aspiration Move':<15}")
        print("-" * 105)

        for iteration in range(maxIT):
            best_move = None
            best_neighbor_obj = float('inf')
            is_aspiration_move = False

            # Evaluate all possible swaps (i, j) where i is in the current solution and j is not
            for i in inList:
                for j in outList:
                    neighbor_solution = inList.copy()
                    neighbor_solution.remove(i)
                    neighbor_solution.add(j)
                    neighbor_obj = compute_objective(neighbor_solution, distances)
                    swap = (i, j)

                    is_tabu_move = is_tabu(stmMatrix, swap, iteration)
                    # Check if the move is allowed (either not tabu or meets the aspiration criterion)
                    if (not is_tabu_move) or (is_tabu_move and neighbor_obj < best_evaluation):
                        if neighbor_obj < best_neighbor_obj:
                            best_move = (neighbor_solution, swap, neighbor_obj)
                            best_neighbor_obj = neighbor_obj
                            is_aspiration_move = is_tabu_move and neighbor_obj < best_evaluation

            if best_move is None:
                print("No non-tabu moves available. Terminating search.")
                break

            best_neighbor_solution, best_swap, best_neighbor_obj = best_move

            if is_aspiration_move:
                aspiration_count += 1

            make_tabu(stmMatrix, best_swap, iteration)  # Mark the best move as tabu

            inList = best_neighbor_solution
            outList = set(range(nRow)) - inList

            # Update the best local solution if the new solution is better
            if best_neighbor_obj < best_local_obj:
                best_local_obj = best_neighbor_obj
                no_improvement_iterations = 0
                last_improvement_obj = best_local_obj
            else:
                no_improvement_iterations += 1

            # Update the global best solution if the new solution is better
            if best_local_obj < best_evaluation:
                best_solution = list(inList)
                best_evaluation = best_local_obj
                best_evaluation_location = (iteration, run)

            # Log the details of the current iteration
            swap_str = f"{best_swap[0]} -> {best_swap[1]}"
            print(f"{iteration:<10}{swap_str:<20}{best_neighbor_obj:<20.2f}{best_local_obj:<20.2f}{best_evaluation:<20.2f}{'Yes' if is_aspiration_move else 'No':<15}")

            run_data.append({
                'Run': run,
                'Iteration': iteration,
                'Objective_Value': best_neighbor_obj,
                'Best_Local_Obj': best_local_obj,
                'Best_Global_Obj': best_evaluation,
                'Is_Aspiration_Move': is_aspiration_move,
                'Is_Greedy_Start': is_greedy
            })

            if no_improvement_iterations >= no_improvement_limit:
                print(f'\nLong-term memory triggered after {no_improvement_iterations} iterations without improvement.')
                print(f'Last improvement objective: {last_improvement_obj:.2f}')
                break

        return {
            'best solution': best_solution,
            'best evaluation': best_evaluation,
            'aspiration count': aspiration_count,
            'best_evaluation_location': best_evaluation_location,
            'is_greedy': is_greedy,
            'starting_solution': solution,
            'run_data': run_data
        }

    def plot_progress(df, best_global, known_optimal, filename, nLOC, maxIT, tenure, runs, no_improvement_limit, seed, penalty_fraction, runtime):
        """
        Plots the progress of the Tabu Search algorithm over all runs.
        
        Parameters:
        df (DataFrame): DataFrame containing the iteration data from all runs.
        best_global (tuple): Tuple containing the best global iteration and its objective value.
        known_optimal (float): Known optimal solution value, if available.
        filename (str): Path to the file containing the cost matrix.
        nLOC (int): Number of locations to select.
        maxIT (int): Maximum number of iterations per run.
        tenure (int): Tabu tenure (number of iterations a move remains tabu).
        runs (int): Number of runs performed.
        no_improvement_limit (int): Maximum number of iterations without improvement before termination.
        seed (int): Random seed for reproducibility.
        penalty_fraction (float): Fraction of the maximum distance used to calculate penalties.
        runtime (float): Total runtime of the algorithm.
        """
        plt.figure(figsize=(14, 8))  # Increased figure size to accommodate more information
        
        for run in df['Run'].unique():
            run_data = df[df['Run'] == run]
            start_type = 'o' if run_data.iloc[0]['Is_Greedy_Start'] else 's'
            plt.plot(run_data['Overall_Iteration'], run_data['Objective_Value'], label=f'Run {run}')
            plt.plot(run_data['Overall_Iteration'].iloc[0], run_data['Objective_Value'].iloc[0], start_type, 
                     markersize=8, markeredgecolor='black', markerfacecolor='none')
        
        plt.plot(best_global[0], best_global[1], 'r*', markersize=15, label=f'Best Solution Found: {best_global[1]:.4f}')
        
        if known_optimal is not None:
            plt.axhline(y=known_optimal, color='g', linestyle='--', label=f'Known Optimal: {known_optimal:.4f}')
        
        plt.xlabel('Overall Iterations')
        plt.ylabel('Objective Value')
        
        # Updated title with all parameters
        title = (f'Tabu Search Progress\n'
                 f'Dataset: {filename}, Locations: {nLOC}\n'
                 f'Parameters: maxIT={maxIT}, tenure: {tenure}, runs: {runs},\n'
                 f'no_improvement_limit: {no_improvement_limit}, seed: {seed}, penalty_fraction: {penalty_fraction}\n'
                 f'Runtime: {runtime:.2f} seconds')
        plt.title(title)
        
        plt.legend()
        plt.grid(True)
        
        # Add legend for start types
        plt.plot([], [], 'o', color='none', markeredgecolor='black', label='Greedy Start')
        plt.plot([], [], 's', color='none', markeredgecolor='black', label='Random Start')
        plt.legend()
        
        plt.tight_layout()  # Adjust layout to prevent cutting off title
        plt.show()

    random.seed(seed)
    distances = read_costMatrix()
    stmMatrix = np.zeros((len(distances), len(distances)), dtype=int)  # Initialize a square array of zeros the same size as the distance matrix
    penalty_matrix = np.zeros_like(distances)  # Initialize the penalty matrix with zeros
    
    max_distance = np.max(distances)  # Find the largest value in the cost matrix
    penalty_value = max_distance * penalty_fraction  # Calculate the penalty value
    print(f"Max distance in cost matrix: {max_distance:.2f}")
    print(f"Penalty value: {penalty_value:.2f}\n")

    best_solution = None
    best_evaluation_location = (0, 0)
    best_evaluation = float('inf')
    previous_greedy_solution = None
    total_swap_percentage = 0
    greedy_comparison_count = 0

    all_runs_data = []
    best_global = (0, float('inf'))

    for run in range(runs):
        print(f'-------------------- Run #{run} --------------------')
        iteration_result = tabu_search(distances, run, best_evaluation_location, best_evaluation, best_solution, stmMatrix, penalty_matrix)
        
        all_runs_data.extend(iteration_result['run_data'])

        if iteration_result['best evaluation'] < best_global[1]:
            best_global = (len(all_runs_data) - 1, iteration_result['best evaluation'])

        if iteration_result['is_greedy']:
            new_greedy_solution = iteration_result['starting_solution']
            if previous_greedy_solution is not None:
                swapped_out = set(previous_greedy_solution) - set(new_greedy_solution)
                swapped_in = set(new_greedy_solution) - set(previous_greedy_solution)
                swapped_count = len(swapped_out)
                swap_percentage = (swapped_count / nLOC) * 100
                total_swap_percentage += swap_percentage
                greedy_comparison_count += 1
                print(f"\nCompared to previous greedy solution:")
                print(f"Swapped locations: {swapped_count}/{nLOC} ({swap_percentage:.2f}%)")
                print(f"Locations swapped out: {sorted(swapped_out)}")
                print(f"Locations swapped in: {sorted(swapped_in)}")
            previous_greedy_solution = new_greedy_solution

        if iteration_result['best evaluation'] < best_evaluation:
            best_evaluation = iteration_result['best evaluation']
            best_solution = iteration_result['best solution']
            best_evaluation_location = iteration_result['best_evaluation_location']
        
        # Update penalty matrix
        for loc in best_solution:
            penalty_matrix[loc, :] += penalty_value
            penalty_matrix[:, loc] += penalty_value

        print(f"\nRun Summary:")
        print(f"Best solution: {best_solution}")
        print(f"Objective value: {best_evaluation:.2f}")
        print(f"Found in iteration {best_evaluation_location[0]} of run {best_evaluation_location[1]}")
        print(f"Aspiration moves: {iteration_result['aspiration count']}\n")

    # After all runs are complete
    df = pd.DataFrame(all_runs_data)
    df['Overall_Iteration'] = df.index

    runtime = time.time() - start_time
    plot_progress(df, best_global, known_optimal, filename, nLOC, maxIT, tenure, runs, no_improvement_limit, seed, penalty_fraction, runtime)

    # Print summary statistics
    print("\nSummary Statistics:")
    print(df.groupby('Run')['Objective_Value'].agg(['min', 'max', 'mean', 'std']))

    print("Final Results:")
    print(f"Best overall solution: {best_solution}")
    print(f"Best overall objective value: {best_evaluation:.2f}")
    print(f"Found in iteration {best_evaluation_location[0]} of run {best_evaluation_location[1]}")
    
    if greedy_comparison_count > 0:
        average_swap_percentage = total_swap_percentage / greedy_comparison_count
        print(f"\nAverage percentage of locations swapped in greedy solutions: {average_swap_percentage:.2f}%")
    else:
        print("\nNo greedy solution comparisons were made.")

    print(f"\nTotal runtime: {runtime:.2f} seconds")

if __name__ == '__main__':
    # Run the tabu search algorithm with specified parameters
    solve_pMedian(
        filename='spd100new.dat',  # File containing the distance matrix
        nLOC=12,                   # Number of locations to select
        maxIT=100,                 # Maximum iterations per run
        tenure=35,                 # Tabu tenure (how long a move remains tabu)
        runs=5,                    # Number of times to run the algorithm
        no_improvement_limit=1000, # Iterations without improvement before triggering long-term memory
        seed=120,                  # Random seed for reproducibility
        penalty_fraction=0.01,     # Fraction of max distance to use as penalty
        known_optimal=4171         # Known optimal solution (if available) for comparison
    )